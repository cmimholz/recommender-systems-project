{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1rh9yuFGqPu"
      },
      "source": [
        "# Recommender System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YIUtatOGqPw"
      },
      "source": [
        "## Import: Mount Google Drive for Data Access\n",
        "In this section, we mount Google Drive to access files stored in your Google Drive account. This is necessary to load datasets or save results during the training process.\n",
        "\n",
        "We are using Google Colab because of its GPU capabilities, which allow us to accelerate the training of machine learning models, especially when working with large datasets or complex models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irUeZYnbGqPx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "577e-Gn2fX0w",
        "outputId": "63a0d038-ccc9-4e2a-dc29-ab9dbfde800d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import the drive module from google.colab to access Google Drive\n",
        "# Mount the Google Drive to the '/content/drive' directory\n",
        "# This will prompt the user to authenticate and give access to their Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIv3Te_YGqPy"
      },
      "source": [
        "# Import training data\n",
        "## Loading and Preparing Dataset for Music Recommendation System\n",
        "In this section, we load the training, validation, and test datasets, as well as supporting data like song and user information, from CSV files stored in Google Drive. The datasets are samples representing users' complete listening histories, where the users were down-sampled, not the music tracks.\n",
        "\n",
        "Key details:\n",
        "- **Train, Validation, Test Sets**: These contain user listening histories, specifically media (music tracks) that users have interacted with. The users have been down-sampled, not the media items.\n",
        "- **Songs**: A list of all unique `media_id` values from the dataset, representing the available music tracks.\n",
        "- **Dummy Users**: A list of all users that appear in the training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81CTP36PGqPy"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Recommender System/data/train_dataset.csv', usecols=['media_id', 'user_id'])\n",
        "valid = pd.read_csv('/content/drive/MyDrive/Recommender System/data/validation_dataset.csv', usecols=['media_id', 'user_id'])\n",
        "test = pd.read_csv('/content/drive/MyDrive/Recommender System/data/test_dataset.csv', usecols=['media_id', 'user_id'])\n",
        "dummy_users = pd.read_csv('/content/drive/MyDrive/Recommender System/data/added_users.csv').values.flatten().astype(str)\n",
        "songs =pd.read_csv('/content/drive/MyDrive/Recommender System/data/media_ids.csv').values.flatten().astype(int)\n",
        "\n",
        "# Rename the columns ('media_id' to 'songId' and 'user_id' to 'dummyUserId')\n",
        "train.rename(columns={'media_id': 'songId', 'user_id': 'dummyUserId'}, inplace=True)\n",
        "valid.rename(columns={'media_id': 'songId', 'user_id': 'dummyUserId'}, inplace=True)\n",
        "test.rename(columns={'media_id': 'songId', 'user_id': 'dummyUserId'}, inplace=True)\n",
        "\n",
        "train['dummyUserId'] = train['dummyUserId'].astype(str)\n",
        "valid['dummyUserId'] = valid['dummyUserId'].astype(str)\n",
        "test['dummyUserId'] = test['dummyUserId'].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "OrcjhgnNhIzV",
        "outputId": "f707d1cf-8e1d-48a0-a3f6-aebdbcc7f855"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f8d482aa-077b-448d-8550-93898b487a9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>productId</th>\n",
              "      <th>dummyUserId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3110394</td>\n",
              "      <td>9829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>71247303</td>\n",
              "      <td>4456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>126773241</td>\n",
              "      <td>734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>112299140</td>\n",
              "      <td>9557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9997030</td>\n",
              "      <td>2073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633380</th>\n",
              "      <td>133782124</td>\n",
              "      <td>1526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633381</th>\n",
              "      <td>133103204</td>\n",
              "      <td>347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633382</th>\n",
              "      <td>125089704</td>\n",
              "      <td>5593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633383</th>\n",
              "      <td>88901389</td>\n",
              "      <td>4589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633384</th>\n",
              "      <td>117331592</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>633385 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8d482aa-077b-448d-8550-93898b487a9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f8d482aa-077b-448d-8550-93898b487a9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f8d482aa-077b-448d-8550-93898b487a9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-65da9044-7529-4036-833a-1d34c1076b4a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-65da9044-7529-4036-833a-1d34c1076b4a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-65da9044-7529-4036-833a-1d34c1076b4a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d77ee7fd-6132-4735-a623-08222ceb71dc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d77ee7fd-6132-4735-a623-08222ceb71dc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        productId dummyUserId\n",
              "0         3110394        9829\n",
              "1        71247303        4456\n",
              "2       126773241         734\n",
              "3       112299140        9557\n",
              "4         9997030        2073\n",
              "...           ...         ...\n",
              "633380  133782124        1526\n",
              "633381  133103204         347\n",
              "633382  125089704        5593\n",
              "633383   88901389        4589\n",
              "633384  117331592         171\n",
              "\n",
              "[633385 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGAY7PKkhMLa",
        "outputId": "c18b503b-a662-4b6a-eb7b-c474f9f5faed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['16385', '4', '16392', ..., '16376', '16378', '8187'], dtype='<U21')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dummy_users # user list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrKM8qFShUxr",
        "outputId": "16356ef2-3318-4b7d-e827-1de831fc80e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 2288934,  6190094,  7410816, ...,   994484, 11522510, 71815668])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "songs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xVmC3ODGqP1"
      },
      "source": [
        "## Defining the Recommender Model: Simple Recommender with Embeddings\n",
        "\n",
        "In this section, we define a simple recommendation model using **embedding layers** and **dot song** similarity to predict how closely related a user is to a song (music track). The goal is to use collaborative filtering techniques to recommend items (tracks) based on user-item interactions.\n",
        "\n",
        "### Approach: Collaborative Filtering with Embeddings and Matrix Factorization\n",
        "\n",
        "The **SimpleRecommender** model is based on **collaborative filtering** and the concept of **matrix factorization**, which is a popular technique in recommendation systems.\n",
        "\n",
        "#### Matrix Factorization:\n",
        "Matrix factorization is a technique that decomposes a large interaction matrix (e.g., user-item interactions or ratings) into two smaller matrices:\n",
        "- One matrix for **users** (user embeddings)\n",
        "- One matrix for **items** (song embeddings)\n",
        "\n",
        "These matrices capture latent factors—hidden user preferences and item attributes—that help predict interactions between users and items. The goal is to learn these embeddings in a way that minimizes the prediction error.\n",
        "\n",
        "In this model:\n",
        "- **Users** are represented by embedding vectors (`user_embedding`).\n",
        "- **songs** (or tracks in the Deezer dataset) are represented by embedding vectors (`song_embedding`).\n",
        "\n",
        "The model computes the **dot song** of these embedding vectors to determine the similarity or relevance between a user and a song, similar to how traditional matrix factorization works. The dot song serves as a prediction for how likely the user is to engage with a song (e.g., listen to a track).\n",
        "\n",
        "#### Embeddings:\n",
        "The model uses **embedding layers** to represent users and songs in a continuous vector space. These embeddings transform discrete IDs (such as user IDs or media IDs) into dense vectors, where similar users and songs have embeddings that are closer to each other in the vector space. The dimensionality of these embeddings is controlled by the `length_of_embedding` parameter.\n",
        "\n",
        "- **User Embeddings**: Each user is represented as a dense vector.\n",
        "- **Song Embeddings**: Each song (track) is also represented as a dense vector.\n",
        "\n",
        "#### Dot Song for Similarity Calculation:\n",
        "The model computes the similarity between a user and a song using the **dot song** of their embedding vectors. The dot song is method in recommender systems to measure the proximity of two vectors. If two vectors are closely aligned (pointing in the same direction), the dot song will yield a higher value, indicating higher similarity. If the vectors are orthogonal or pointing in different directions, the dot song will be lower, indicating less similarity.\n",
        "\n",
        "**Mathematically**:\n",
        "Given a user embedding vector \\( u \\) and a song embedding vector \\( p \\), the similarity score \\( s \\) between the user and song is calculated as:\n",
        "\n",
        "$$\n",
        "s(u, p) = u \\cdot p = \\sum_{i=1}^{n} u_i \\times p_i\n",
        "$$\n",
        "\n",
        "Where \\( n \\) is the dimensionality of the embedding (i.e., the length of the embedding vector).\n",
        "\n",
        "### Finding Similar songs:\n",
        "The model includes a `call_item_item` function that allows us to find the most similar songs (tracks) to a given song. This is done by comparing the embedding of the target song to the embeddings of all other songs in the dataset. The top 100 songs with the highest dot song similarity scores are then returned.\n",
        "\n",
        "In the upcoming sections, we will explore how this model is trained using the Deezer dataset to predict user behavior and recommend tracks that users are likely to listen to for more than 30 seconds without skipping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ3GOvuuGqP1"
      },
      "outputs": [],
      "source": [
        "class SimpleRecommender(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    A simple recommendation model that uses embeddings for users and songs and\n",
        "    calculates similarity scores using the dot song of their embeddings.\n",
        "\n",
        "    Args:\n",
        "        dummy_users (list of str): A list of user identifiers (e.g., user names or IDs).\n",
        "        songs (list of int): A list of song identifiers (e.g., song IDs).\n",
        "        length_of_embedding (int): The size of the embedding vector for both users and songs.\n",
        "\n",
        "    Attributes:\n",
        "        songs (tf.Tensor): A constant tensor storing the song IDs.\n",
        "        dummy_users (tf.Tensor): A constant tensor storing the user identifiers.\n",
        "        dummy_user_table (tf.lookup.StaticHashTable): A lookup table mapping user identifiers\n",
        "            to embedding indices.\n",
        "        song_table (tf.lookup.StaticHashTable): A lookup table mapping song IDs to\n",
        "            embedding indices.\n",
        "        user_embedding (tf.keras.layers.Embedding): Embedding layer for users.\n",
        "        song_embedding (tf.keras.layers.Embedding): Embedding layer for songs.\n",
        "        dot (tf.keras.layers.Dot): Layer for calculating the dot song between user and\n",
        "            song embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, dummy_users, songs,length_of_embedding):\n",
        "        \"\"\"\n",
        "        Initializes the SimpleRecommender model with user and song data, and creates the\n",
        "        necessary embedding layers and lookup tables.\n",
        "\n",
        "        Args:\n",
        "            dummy_users (list of str): List of user identifiers.\n",
        "            songs (list of int): List of song identifiers.\n",
        "            length_of_embedding (int): Length of the embedding vector for both users and songs.\n",
        "        \"\"\"\n",
        "        super(SimpleRecommender, self).__init__()\n",
        "\n",
        "        # Store songs and dummy users as constant tensors\n",
        "        self.songs = tf.constant(songs, dtype=tf.int32)\n",
        "        self.dummy_users = tf.constant(dummy_users, dtype=tf.string)\n",
        "\n",
        "        # Create lookup tables for users and songs, mapping IDs to indices\n",
        "        self.dummy_user_table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(self.dummy_users, range(len(dummy_users))), -1)\n",
        "        self.song_table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(self.songs, range(len(songs))), -1)\n",
        "\n",
        "        # Create embedding layers for users and songs\n",
        "        self.user_embedding = tf.keras.layers.Embedding(len(dummy_users), length_of_embedding)\n",
        "        self.song_embedding = tf.keras.layers.Embedding(len(songs), length_of_embedding)\n",
        "\n",
        "        # Initialize a dot song layer for computing the similarity between embeddings\n",
        "        self.dot = tf.keras.layers.Dot(axes=-1) #Dot is also a Layer\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass of the model. Given a user and a set of songs, it retrieves the\n",
        "        embeddings for both and computes their dot song (similarity).\n",
        "\n",
        "        Args:\n",
        "            inputs (tuple): A tuple containing two elements:\n",
        "                - user (tf.Tensor): A tensor containing the user identifier(s).\n",
        "                - songs (tf.Tensor): A tensor containing the song identifier(s).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: A tensor of similarity scores between the user and each song.\n",
        "        \"\"\"\n",
        "        user = inputs[0]\n",
        "        songs = inputs[1]\n",
        "\n",
        "        # Lookup the indices for user and songs\n",
        "        user_embedding_index = self.dummy_user_table.lookup(user)\n",
        "        song_embedding_index = self.song_table.lookup(songs)\n",
        "\n",
        "        # Retrieve the embedding vectors for the user and songs\n",
        "        user_embedding_values = self.user_embedding(user_embedding_index)\n",
        "        song_embedding_values = self.song_embedding(song_embedding_index)\n",
        "\n",
        "        # Return the dot song of the user and song embeddings (similarity scores)\n",
        "        return tf.squeeze(self.dot([user_embedding_values, song_embedding_values]),1)\n",
        "\n",
        "\n",
        "    def call_item_item(self, song):\n",
        "      \"\"\"\n",
        "      Finds the top 100 songs that are most similar to a given song based on their embeddings.\n",
        "\n",
        "      Args:\n",
        "        song (tf.Tensor or int): A song identifier (song number) for which to find similar songs.\n",
        "\n",
        "      Returns:\n",
        "        top_ids (tf.Tensor): A tensor containing the IDs of the top 100 most similar songs.\n",
        "        top_scores (tf.Tensor): A tensor containing the similarity scores for the top 100 songs.\n",
        "\n",
        "      \"\"\"\n",
        "      song_x = self.song_table.lookup(song)\n",
        "      pe = tf.expand_dims(self.song_embedding(song_x), 0)\n",
        "\n",
        "      all_pe = tf.expand_dims(self.song_embedding.embeddings, 0)#note this only works if the layer has been built!\n",
        "      scores = tf.reshape(self.dot([pe, all_pe]), [-1])\n",
        "\n",
        "      top_scores, top_indices = tf.math.top_k(scores, k=100)\n",
        "      top_ids = tf.gather(self.songs, top_indices)\n",
        "      return top_ids, top_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmDuu9GAGqP2"
      },
      "source": [
        "# Creating a dataset\n",
        "## Negative Sampling for Recommender Model\n",
        "\n",
        "In this section, we introduce the **Mapper** class and the **get_dataset** function, which are used for **negative sampling** during training.\n",
        "\n",
        "### Negative Sampling in Recommendation Systems:\n",
        "In recommendation systems, the model learns from both positive and negative examples. Positive examples are the songs (tracks) that users have interacted with, while negative examples are songs the users have not interacted with. Since there are far more songs that a user has not interacted with, it's inefficient to use all possible songs as negative examples. Instead, we use **negative sampling**, where a small subset of non-interacted songs are sampled to train the model.\n",
        "\n",
        "The **Mapper** class is responsible for generating negative samples for each user-song pair. For each positive interaction, a specified number of negative (non-relevant) songs are sampled. The positive and negative songs are combined into a list of candidate songs, and a one-hot encoded label is created to indicate the position of the positive song.\n",
        "\n",
        "The **get_dataset** function prepares the data by mapping each user to a list of positive and negative songs and batching the data for efficient training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQYmEb5eGqP2"
      },
      "outputs": [],
      "source": [
        "class Mapper():\n",
        "    \"\"\"\n",
        "    Maps a certain amount of negative songs to the song. One list which has both values\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    A class used to map a song to a certain number of negative (non-relevant) songs for training\n",
        "    in recommendation systems, typically used in tasks such as negative sampling. It returns a list of\n",
        "    candidate songs containing one positive song and several negative songs, along with\n",
        "    a one-hot encoded label indicating the positive song.\n",
        "\n",
        "    Args:\n",
        "        possible_songs (list of int): A list of all possible song identifiers (e.g., song IDs).\n",
        "        num_negative_songs (int): The number of negative songs to sample for each positive song.\n",
        "\n",
        "    Attributes:\n",
        "        num_possible_songs (int): The total number of possible songs available for negative sampling.\n",
        "        possible_songs_tensor (tf.Tensor): A constant tensor that stores all possible song IDs\n",
        "            for efficient sampling.\n",
        "        num_negative_songs (int): The number of negative (non-relevant) songs to sample.\n",
        "        y (tf.Tensor): A one-hot encoded tensor where the first index is `1` (indicating the positive\n",
        "            song) and all other indices are `0` (indicating negative songs).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, possible_songs, num_negative_songs):\n",
        "        \"\"\"\n",
        "        Initializes the Mapper with the list of possible songs and the number of negative songs to sample.\n",
        "\n",
        "        Args:\n",
        "            possible_songs (list of int): List of all possible song IDs.\n",
        "            num_negative_songs (int): Number of negative songs to sample for each positive song.\n",
        "        \"\"\"\n",
        "        self.num_possible_songs = len(possible_songs)\n",
        "        self.possible_songs_tensor = tf.constant(possible_songs, dtype=tf.int32)\n",
        "\n",
        "        self.num_negative_songs = num_negative_songs\n",
        "        self.y = tf.one_hot(0, num_negative_songs + 1)\n",
        "\n",
        "    def __call__(self, user, song):\n",
        "        \"\"\"\n",
        "        When called, this method samples negative songs for the given positive song and user.\n",
        "        It returns a list of candidate songs consisting of one positive song and several\n",
        "        negative songs, along with a one-hot encoded label indicating the position of the positive song.\n",
        "\n",
        "        Args:\n",
        "            user (tf.Tensor): A tensor representing the user identifier.\n",
        "            song (tf.Tensor): A tensor representing the positive song identifier.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - (user, candidates) (tuple):\n",
        "                    - user (tf.Tensor): The input user tensor.\n",
        "                    - candidates (tf.Tensor): A tensor containing one positive song and multiple\n",
        "                      negative songs concatenated together.\n",
        "                - y (tf.Tensor): A one-hot encoded label where the first position corresponds to the\n",
        "                  positive song, and the rest are zeros for the negative songs.\n",
        "        \"\"\"\n",
        "        random_negatives_indexs = tf.random.uniform((self.num_negative_songs,), minval=0, maxval = self.num_possible_songs, dtype = tf.int32)\n",
        "        negatives = tf.gather(self.possible_songs_tensor, random_negatives_indexs) #maps the indexes to the actual values\n",
        "        candidates = tf.concat([song, negatives], axis=0)\n",
        "        return (user, candidates), self.y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJNkuVcAGqP2"
      },
      "outputs": [],
      "source": [
        "def get_dataset(df, songs, num_negative_songs):\n",
        "    \"\"\"\n",
        "    Prepares a TensorFlow dataset for training a recommendation system model by mapping users to their\n",
        "    respective songs and generating negative samples for each song. This dataset is used to pass\n",
        "    data into the model for training in batches.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): A DataFrame containing user-song interaction data. The DataFrame must\n",
        "            include at least two columns:\n",
        "            - \"dummyUserId\": User identifiers.\n",
        "            - \"songId\": song identifiers.\n",
        "        songs (list of int): A list of all possible song identifiers for negative sampling.\n",
        "        num_negative_songs (int): The number of negative songs to sample for each positive song.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A TensorFlow dataset where each element consists of a tuple:\n",
        "            - (user, candidates):\n",
        "                - user (tf.Tensor): A tensor representing the user identifier.\n",
        "                - candidates (tf.Tensor): A tensor containing one positive song and multiple\n",
        "                  negative songs concatenated together.\n",
        "            - y (tf.Tensor): A one-hot encoded tensor where the first position corresponds to the\n",
        "              positive song, and the rest are zeros for the negative songs.\n",
        "    \"\"\"\n",
        "    dummy_user_tensor = tf.constant(df[[\"dummyUserId\"]].values, dtype=tf.string) # takes all user id and creates a constant tensor\n",
        "    song_tensor = tf.constant(df[[\"songId\"]].values, dtype=tf.int32) #same with songs\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dummy_user_tensor, song_tensor)) # dataset to pass into our model\n",
        "    dataset = dataset.map(Mapper(songs, num_negative_songs))\n",
        "    dataset = dataset.batch(1024)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrAHIwYtGqP3"
      },
      "source": [
        "# Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "558vWlwQGqP3"
      },
      "source": [
        "## Model Compilation and Training\n",
        "\n",
        "In this section, we compile and train the **SimpleRecommender** model using TensorFlow/Keras. The recommendation problem is framed as a classification problem, where the model learns to predict the correct song (positive sample) among several negative samples.\n",
        "\n",
        "### Model Compilation:\n",
        "- **Loss Function (`CategoricalCrossentropy`)**:\n",
        "    - The **categorical cross-entropy** loss is used to measure the difference between the true distribution (which contains one positive song and several negative songs) and the predicted distribution (output of the model).\n",
        "    - The model outputs logits (raw predictions), and the softmax function is applied to convert these logits into probabilities over the song classes. The goal is to maximize the probability assigned to the positive song while minimizing the probability of the negative songs.\n",
        "    \n",
        "    Mathematically, the categorical cross-entropy for a single training example is given by:\n",
        "    \n",
        "  $$\n",
        "  L = - \\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
        "  $$\n",
        "\n",
        "    Where:\n",
        "    - $ y_i $ is the true label (1 for the positive song, 0 for negative songs).\n",
        "    - $\\hat{y}_i$  is the predicted probability for the $( i )$-th song (after softmax).\n",
        "    - $( C )$ is the number of candidate songs (positive + negatives).\n",
        "    \n",
        "    - `from_logits=True` indicates that the model’s output is not passed through a softmax function yet, and it will be applied internally by the loss function.\n",
        "  \n",
        "- **Optimizer (`SGD - Stochastic Gradient Descent`)**:\n",
        "    - **SGD** is used to update the model's weights based on the gradients of the loss function with respect to the model parameters. It makes updates to the weights to minimize the loss during training.\n",
        "    - The learning rate $\\eta = 100$ controls the step size for each update. Although this value is unusually large, it gave good results in this case, suggesting that the model is able to effectively learn with this high step size. Nonetheless, hyperparameter tuning could be required in future application.\n",
        "    \n",
        "$$\n",
        "  w_{new} = w_{old} - \\eta \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "\n",
        "- **Metrics (`CategoricalAccuracy`)**:\n",
        "    - **Categorical accuracy** evaluates the model by checking how often the highest predicted probability (the predicted class) corresponds to the true class (positive song). This metric is crucial for tracking the model's performance in correctly identifying the positive song among the candidates.\n",
        "  \n",
        "### Callbacks for Early Stopping and Model Checkpointing:\n",
        "- **EarlyStopping**: This callback stops training early if the validation loss does not improve for 3 consecutive epochs. It helps to prevent overfitting and ensures that training does not continue if no progress is made.\n",
        "- **ModelCheckpoint**: This callback saves the model that achieves the lowest validation loss during training. By storing the best-performing model, we ensure that we can later use the model that generalizes best to unseen data.\n",
        "\n",
        "### Training the Model:\n",
        "The model is trained on the training dataset, and the validation set is used to monitor the performance. We use 50 epochs with early stopping and model checkpointing to ensure efficient training and prevent overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPdRGOVNGqP3",
        "outputId": "b224d44e-9ba5-4751-babf-2e837b495efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - categorical_accuracy: 0.3531 - loss: 2.5277\n",
            "Epoch 1: val_loss improved from inf to 1.09462, saving model to /content/drive/MyDrive/Recommender System1.1/Recommender System/best_model.keras\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 66ms/step - categorical_accuracy: 0.3534 - loss: 2.5266 - val_categorical_accuracy: 0.7174 - val_loss: 1.0946\n",
            "Epoch 2/50\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - categorical_accuracy: 0.7404 - loss: 0.9763\n",
            "Epoch 2: val_loss improved from 1.09462 to 0.88328, saving model to /content/drive/MyDrive/Recommender System1.1/Recommender System/best_model.keras\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 61ms/step - categorical_accuracy: 0.7405 - loss: 0.9761 - val_categorical_accuracy: 0.7609 - val_loss: 0.8833\n",
            "Epoch 3/50\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - categorical_accuracy: 0.8103 - loss: 0.6517\n",
            "Epoch 3: val_loss improved from 0.88328 to 0.82071, saving model to /content/drive/MyDrive/Recommender System1.1/Recommender System/best_model.keras\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 68ms/step - categorical_accuracy: 0.8103 - loss: 0.6516 - val_categorical_accuracy: 0.7841 - val_loss: 0.8207\n",
            "Epoch 4/50\n",
            "\u001b[1m618/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - categorical_accuracy: 0.8607 - loss: 0.4539\n",
            "Epoch 4: val_loss improved from 0.82071 to 0.81801, saving model to /content/drive/MyDrive/Recommender System1.1/Recommender System/best_model.keras\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 71ms/step - categorical_accuracy: 0.8607 - loss: 0.4538 - val_categorical_accuracy: 0.7941 - val_loss: 0.8180\n",
            "Epoch 5/50\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - categorical_accuracy: 0.8939 - loss: 0.3394\n",
            "Epoch 5: val_loss did not improve from 0.81801\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 75ms/step - categorical_accuracy: 0.8939 - loss: 0.3394 - val_categorical_accuracy: 0.7983 - val_loss: 0.8325\n",
            "Epoch 6/50\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - categorical_accuracy: 0.9119 - loss: 0.2800\n",
            "Epoch 6: val_loss did not improve from 0.81801\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 68ms/step - categorical_accuracy: 0.9119 - loss: 0.2799 - val_categorical_accuracy: 0.8002 - val_loss: 0.8438\n",
            "Epoch 7/50\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - categorical_accuracy: 0.9237 - loss: 0.2409\n",
            "Epoch 7: val_loss did not improve from 0.81801\n",
            "\u001b[1m619/619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 69ms/step - categorical_accuracy: 0.9237 - loss: 0.2408 - val_categorical_accuracy: 0.8021 - val_loss: 0.8556\n",
            "Epoch 7: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79a4247c8c40>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SimpleRecommender(dummy_users, songs, 15)\n",
        "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.SGD(learning_rate=100.),\n",
        "              metrics= ['categorical_accuracy']) # loss function turns the problem into a classification problem.\n",
        "\n",
        "\"\"\"\n",
        "Compiles the SimpleRecommender model by specifying the loss function, optimizer, and evaluation metric.\n",
        "\n",
        "- **Loss function (`CategoricalCrossentropy`)**:\n",
        "    - This loss function is used for multi-class classification problems. In this case, it converts the recommendation problem into a classification task where the model tries to predict the correct song (positive sample) among several negative samples.\n",
        "    - `from_logits=True` means that the model's outputs are raw logits (unnormalized predictions), and the softmax will be applied internally to these logits before computing the cross-entropy loss.\n",
        "\n",
        "- **Optimizer (`SGD - Stochastic Gradient Descent`)**:\n",
        "    - The optimizer used to update the model's weights based on the gradients computed from the loss function.\n",
        "    - `learning_rate=100.` is the learning rate, which controls the size of the steps taken during gradient descent updates. A large learning rate like 100 is unusual and suggests that further fine-tuning or experimentation is expected for stable learning.\n",
        "\n",
        "- **Metrics (`CategoricalAccuracy`)**:\n",
        "    - This metric evaluates the classification accuracy, which measures how often the model's predicted class (the song with the highest score) matches the true class (the actual song).\n",
        "    - Categorical accuracy is used in multi-class classification tasks, where each sample belongs to one of several classes (in this case, one of several songs).\n",
        "\n",
        "Overall, this compilation step prepares the model for training, specifying the objective (minimizing classification error with categorical cross-entropy), the optimization strategy (SGD), and the metric to track (categorical accuracy).\n",
        "\"\"\"\n",
        "\n",
        "# EarlyStopping callback to stop training if validation loss doesn't improve for 3 epochs\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
        "                               mode='min',  # Minimizing validation loss\n",
        "                               restore_best_weights=True,  # Restore model weights from the epoch with the best validation loss\n",
        "                               verbose=1,\n",
        "                               )\n",
        "\n",
        "# ModelCheckpoint to save the model with the best validation loss\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/Recommender System1.1/Recommender System/best_model.keras',  # Filepath to save the best model\n",
        "                             monitor='val_loss',  # Monitor validation loss\n",
        "                             save_best_only=True,  # Save only the best model\n",
        "                             mode='min',  # Model with minimum validation loss is saved\n",
        "                             verbose=1)\n",
        "\n",
        "\n",
        "model.fit(get_dataset(train, songs, 20),validation_data=get_dataset(valid,songs,20), epochs=50, callbacks=[early_stopping, checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5NwDjLA0xVK"
      },
      "source": [
        "## Training Results Interpretation:\n",
        "\n",
        "- **Epoch 1**:\n",
        "  - **Training accuracy**: 35.31%, **Training loss**: 2.5277\n",
        "  - **Validation accuracy**: 71.74%, **Validation loss**: 1.0946\n",
        "  - **Interpretation**: The model starts with relatively low training accuracy (35.31%), which is typical in the initial epoch as the model starts learning. The validation accuracy is significantly higher (71.74%), indicating that the model generalizes quite well even in the early stages of training. The model is saved after this epoch since the validation loss improves from infinity to 1.0946.\n",
        "\n",
        "- **Epoch 2**:\n",
        "  - **Training accuracy**: 74.04%, **Training loss**: 0.9763\n",
        "  - **Validation accuracy**: 76.09%, **Validation loss**: 0.8833\n",
        "  - **Interpretation**: Both training and validation accuracy show substantial improvement. The model is still generalizing well, and the validation accuracy remains higher than the training accuracy. The model is saved as the validation loss improves to 0.8833.\n",
        "\n",
        "- **Epoch 3**:\n",
        "  - **Training accuracy**: 81.03%, **Training loss**: 0.6517\n",
        "  - **Validation accuracy**: 78.41%, **Validation loss**: 0.8207\n",
        "  - **Interpretation**: The model continues to improve, with training accuracy increasing to 81.03% and validation accuracy reaching 78.41%.\n",
        "\n",
        "- **Epoch 4**:\n",
        "  - **Training accuracy**: 86.07%, **Training loss**: 0.4539\n",
        "  - **Validation accuracy**: 79.41%, **Validation loss**: 0.8180\n",
        "  - **Interpretation**: The training accuracy rises to 86.07%, and the validation accuracy improves slightly to 79.41%. The validation loss plateaus, indicating that the model might be approaching its peak performance. The model is saved once again since the validation loss reaches its lowest point (0.8180).\n",
        "\n",
        "- **Epochs 5-7**:\n",
        "  - The training accuracy continues to improve, reaching 92.37% by epoch 7, but validation loss stops improving, indicating the onset of overfitting. Early stopping is triggered after three consecutive epochs with no further improvement in validation loss, and the model restores the weights from the best epoch (epoch 4).\n",
        "\n",
        "### Why Validation Accuracy is Higher than Training Accuracy:\n",
        "\n",
        "It’s unusual but possible that validation accuracy is higher than training accuracy during the initial stages of training. One reason for this include:\n",
        "1. **Data Distribution**: The validation data might, by chance, be easier for the model to predict than the training data. This can occur if the training data is more diverse or contains more noise.\n",
        "3. **Early Stages of Training**: In early epochs, the model might be underfitting the training data but generalizing better to the validation set. Over time, as the model continues to learn, the training accuracy usually catches up and surpasses validation accuracy.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The model reaches its best performance at **epoch 4**:\n",
        "- **Training accuracy**: 86.07%\n",
        "- **Validation accuracy**: 79.41%\n",
        "- **Validation loss**: 0.8180\n",
        "\n",
        "Early stopping prevents further overfitting, and the model reverts to the best weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOJ693jRyJUz",
        "outputId": "2c79d3a1-8f9a-448a-96e3-7748b9b6c744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - categorical_accuracy: 0.7940 - loss: 0.8134\n",
            "Test Loss: 0.8117275834083557\n",
            "Test Categorical Accuracy: 0.7946804761886597\n"
          ]
        }
      ],
      "source": [
        "# After training, loads the best saved model:\n",
        "#best_model = tf.keras.models.load_model('/content/drive/MyDrive/Recommender System1.1/Recommender System/best_model.keras')\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_categorical_accuracy = model.evaluate(get_dataset(test, songs, 20))\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Categorical Accuracy: {test_categorical_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkOQvTJysYx0"
      },
      "source": [
        "### Test Results on Unseen Data\n",
        "\n",
        "After training, the best model was loaded and evaluated on an unseen test dataset. The model performed as follows:\n",
        "\n",
        "- **Test Loss**: 0.8117\n",
        "- **Test Categorical Accuracy**: 79.47%\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "- The **Test Categorical Accuracy** of 79.47% indicates that the model generalizes well on completely unseen data. This accuracy is consistent with the validation accuracy of 79.41%, showing that the model maintains similar performance across both validation and test sets.\n",
        "  \n",
        "- The **Test Loss** of 0.8117, which is very close to the validation loss, demonstrates that the model's predictions on the unseen test set are reliable and consistent with its performance during training.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The model demonstrates strong generalization capabilities, achieving a categorical accuracy of nearly **80%** on unseen test data. This suggests that the model is effective in classifying and recommending relevant items for users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4KcovzXW4vU"
      },
      "source": [
        "The test accuracy of **79.47%** in your model is highly competitive. Traditional **collaborative filtering** systems typically achieve around **60-80%** accuracy, while **hybrid models** and **deep learning-based recommenders** can reach **80-90%**. Our result is in line with top-performing recommender systems that use similar techniques, particularly those leveraging embeddings and matrix factorization\n",
        "\n",
        "Sources:\n",
        "- [Source 1: A systematic review on recommender systems](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592-5)\n",
        "- [Source 2: Recommender systems metrics and challenges](https://www.mdpi.com/2504-4990/1/1/2)\n",
        "\n",
        "\n",
        "However, due to the high accuracy result, it is important to thoroughly review the entire process to ensure no mistakes were made. High accuracy can sometimes indicate potential issues like overfitting, data leakage, or unintended biases in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va6xrWWzYFbo"
      },
      "source": [
        "## Item-Item Recommendations for a Specific Song\n",
        "\n",
        "In this section, we use the trained model to generate recommendations for a specific song (identified by its media ID: **2288934**). The model uses the embeddings learned during training to compute the similarity between the selected song and other songs in the dataset. The function `call_item_item` returns the top 100 most similar songs based on the dot song of their embeddings.\n",
        "\n",
        "- The first tensor represents the **media IDs** of the recommended songs.\n",
        "- The second tensor represents the corresponding **similarity scores** between the input song and the recommended songs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VX_QUzpGqP3"
      },
      "outputs": [],
      "source": [
        "test_song = 2288934"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBzykv4wGqP3",
        "outputId": "39b3fcc9-6597-4229-9b5b-9cb228b2fd39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recs for item 2288934: (<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
            "array([ 65343735,  96116654,  12216347,  96116652, 110944944, 132694550,\n",
            "        96116656, 132327396,  68299519, 129610550,  65644373,  65343736,\n",
            "        79846356,    574651,  68299514,  63571708, 108707196,  75622512,\n",
            "        91635642,  63571711,  63571702,  12216349, 108707198, 110954658,\n",
            "        65087586,  78003847,  65343737,  65343731,  96116620,  65087601,\n",
            "        63571715,  65087607,  65087592,   3384364, 113531926,  63571703,\n",
            "       110944936, 129253268,  67350374,    574723, 110944942, 105648250,\n",
            "        69416820,  66243613,  68299534,  67350376, 129253266,  71961315,\n",
            "        92243206,  65087597, 110944956, 105184718,  67350369, 118994488,\n",
            "        66317202, 105976014,  77501078,  67234889,  92516866,  67350384,\n",
            "         2605014,   2605024,  72941883, 105184712, 105184732,  96116610,\n",
            "        69416814, 110944922,    574654,  67350382,   2605019,  65087595,\n",
            "        67350373, 121653542,  65087605, 110944962,  67350380,  92537918,\n",
            "         2605016,  67350365, 105648252,    574663, 127794589,  96116618,\n",
            "        92537544,   6427346,   7863292,  66243614, 110944930,    574650,\n",
            "          757815,  63571713, 120681756, 110944954, 104238668,  67350383,\n",
            "        78003851,  63571709, 117379666,  65087593], dtype=int32)>, <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
            "array([0.26256308, 0.25702927, 0.2526977 , 0.2456068 , 0.24559608,\n",
            "       0.24498005, 0.24193497, 0.23427302, 0.23139633, 0.22357558,\n",
            "       0.21355268, 0.20788161, 0.20644593, 0.19388631, 0.19354597,\n",
            "       0.19207038, 0.19191866, 0.19186157, 0.19151452, 0.19135197,\n",
            "       0.19093324, 0.190697  , 0.1891293 , 0.18837066, 0.18714957,\n",
            "       0.18672411, 0.18602669, 0.1838027 , 0.18351085, 0.18281113,\n",
            "       0.18258668, 0.17883588, 0.17800823, 0.17747705, 0.17590371,\n",
            "       0.17499582, 0.17470475, 0.17412004, 0.17381287, 0.17226726,\n",
            "       0.17191799, 0.17180689, 0.17134708, 0.1712795 , 0.17120813,\n",
            "       0.17110711, 0.17090537, 0.17012812, 0.16965486, 0.16905792,\n",
            "       0.16870427, 0.16864662, 0.16855302, 0.16839777, 0.16834192,\n",
            "       0.16830055, 0.16807215, 0.16807173, 0.16803747, 0.16777754,\n",
            "       0.16773358, 0.16752167, 0.16735338, 0.16710913, 0.16672063,\n",
            "       0.16659418, 0.16652398, 0.16583891, 0.16583553, 0.1657595 ,\n",
            "       0.16574898, 0.16574681, 0.16566132, 0.16554245, 0.16553804,\n",
            "       0.16544098, 0.16540493, 0.16534576, 0.16510503, 0.16503917,\n",
            "       0.16502008, 0.16500996, 0.16495971, 0.16493396, 0.16481952,\n",
            "       0.16471817, 0.16464147, 0.16460066, 0.1644756 , 0.16442859,\n",
            "       0.16441599, 0.1643673 , 0.1642013 , 0.16406545, 0.16383055,\n",
            "       0.16382445, 0.16380005, 0.16371521, 0.1636757 , 0.16338404],\n",
            "      dtype=float32)>)\n"
          ]
        }
      ],
      "source": [
        "print(\"Recs for item {}: {}\".format(test_song, model.call_item_item(tf.constant(test_song, dtype=tf.int32))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tH3ZB_x3xeu"
      },
      "source": [
        "Here are the top 5 most similar songs:\n",
        "1. **65343735** with a similarity score of **0.26256308**\n",
        "2. **96116654** with a similarity score of **0.25702927**\n",
        "3. **12216347** with a similarity score of **0.2526977**\n",
        "4. **110944944** with a similarity score of **0.2456068**\n",
        "5. **132694550** with a similarity score of **0.24559608**\n",
        "\n",
        "These results demonstrate the model's ability to recommend tracks that are closely related to the input track based on learned embeddings and their dot song similarities. The higher the score, the more similar the recommended song is to the input song.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x_Ay87QY8WK"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "The recommender model achieved a strong validation accuracy of **87.39%**, demonstrating its effectiveness in making personalized recommendations based on learned embeddings and dot-song similarity. This high accuracy places the model on par with competitive collaborative filtering and matrix factorization-based systems. However, due to the high result, it is essential to thoroughly verify the entire process to ensure no mistakes were made, such as overfitting or data leakage.\n",
        "\n",
        "In addition to the validation accuracy, it would be beneficial to further evaluate the model using other important metrics, such as **Precision@K**. Precision@K is commonly used in recommender systems to measure the relevance of the top K recommended items. It can be calculated using the formula:\n",
        "\n",
        "$$\n",
        "\\text{Precision@K} = \\frac{\\text{Number of Relevant Items in Top K}}{K}\n",
        "$$\n",
        "\n",
        "This would provide additional insights into how well the model performs in ranking and recommending the most relevant items for the user, allowing for a more comprehensive evaluation of its performance.\n",
        "\n",
        "Further exploration using other metrics, such as **Recall@K**, **Mean Average Precision (MAP)**, or **NDCG**, could also help refine the model's effectiveness in practical settings.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
